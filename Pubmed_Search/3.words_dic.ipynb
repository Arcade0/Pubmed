{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逐个倒排索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐个索引\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "files = sorted([\"pubmed_2022d/\" + ele for ele in os.listdir(\"pubmed_2022d\") if \"json\" in ele])\n",
    "\n",
    "spec_papers_d = {}\n",
    "for i in [\"Bactology\", \"Mycology\", \"Virology\"]:\n",
    "    with open(\"specs_list/%s.txt\" % (i), \"r\") as f:\n",
    "        specs = [i.replace(\"\\\"\", \"\").replace(\"\\n\", \"\").lower() for i in f.readlines()]\n",
    "    spec_paper_d = {}\n",
    "    for spec in specs:\n",
    "        spec_paper_d[spec] = {}\n",
    "    spec_papers_d[i] = spec_paper_d\n",
    "\n",
    "done_l = []\n",
    "\n",
    "# 读取pubmed文件\n",
    "for i, file in enumerate(files):\n",
    "\n",
    "    # 选取没有被处理的\n",
    "    if file not in done_l:\n",
    "    \n",
    "        print(file)\n",
    "\n",
    "        with open(file, 'r') as f:\n",
    "            papers = json.load(f)\n",
    "        \n",
    "        # 分割成小字典\n",
    "        papers_l = []\n",
    "        papers_sub = {}\n",
    "        for l, (k, v) in enumerate(papers.items()):\n",
    "            papers_sub[k] = v\n",
    "            if l in range(999, len(papers)+1000, 1000):\n",
    "                papers_l.append(papers_sub)\n",
    "                papers_sub = {}\n",
    "        dics = {}\n",
    "\n",
    "        # 每个小字典\n",
    "        for j, papers in enumerate(papers_l):\n",
    "            \n",
    "            dic = {}\n",
    "            \n",
    "            # 每个paper\n",
    "            for k, line in papers.items():\n",
    "\n",
    "                text = line[\"texts\"][\"Ab\"] +\" \"+\" \".join(sum(line[\"Meshhead\"],[])) \n",
    "                paper = text.lower()\n",
    "                paper = paper.translate(str.maketrans(string.punctuation, \" \"*32,)).replace(\"  \", \" \")\n",
    "                word_l = paper.split(\"\")\n",
    "                words = set(word_l)\n",
    "\n",
    "                for word in words:\n",
    "                    if word in dic:\n",
    "                        dic[word] = dic[word] + [line]\n",
    "                    else:\n",
    "                        dic[word] = [line]\n",
    "            \n",
    "            for k, v in dic.items():\n",
    "                dics.setdefault(k,[]).extend(v)\n",
    "\n",
    "        # 每类物种检索\n",
    "        for spec_type in spec_papers_d:\n",
    "    \n",
    "            # 每种物种\n",
    "            for spec in spec_papers_d[spec_type]:\n",
    "                \n",
    "                terms = spec.lower().split(\" \")\n",
    "                term_paper = dics.setdefault(terms[0], [0])\n",
    "                \n",
    "                for term in terms:\n",
    "                    \n",
    "                    term_paper = [i for i in term_paper if i in dics.setdefault(term, [0])]\n",
    "\n",
    "                if 0 in term_paper:\n",
    "                    term_paper.remove(0)\n",
    "            \n",
    "            term_paper_dic = {}\n",
    "            for paper in term_paper:\n",
    "                term_paper_dic[term[\"pmid\"]] = term\n",
    "\n",
    "            spec_papers_d[spec_type][spec].update(term_paper_dic)\n",
    "    \n",
    "        done_l.append(file)\n",
    "\n",
    "        if i in [300,600,900,len(files)-1]:\n",
    "            with open(\"search_results/search_results_map.txt\", 'w') as f:\n",
    "                content = json.dumps(spec_papers_d)\n",
    "                f.write(content)\n",
    "            with open(\"search_results/search_done.json\", 'w') as f:\n",
    "                json.dump(done_l, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 倒排索引建立大表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立词表\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import time\n",
    "\n",
    "files = [\"pubmed_2022d/\" + ele for ele in os.listdir(\"pubmed_2022d\") if \"json\" in ele]\n",
    "files.sort()\n",
    "\n",
    "if os.path.exists(\"words_dic/words_dic_done.json\") is False:\n",
    "    done_l = []\n",
    "    print(\"done_l is empty\")\n",
    "else:\n",
    "    with open(\"words_dic/words_dic_done.json\") as f:\n",
    "        done_l = json.load(f)\n",
    "        print(\"done_l is exists\")\n",
    "        f.close()\n",
    "invert_dic = {}\n",
    "files = [i for i in files if i not in done_l]\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "\n",
    "    print(file)\n",
    "    with open(file, 'r') as f:\n",
    "        papers = json.load(f)\n",
    "        f.close()\n",
    "            \n",
    "    zerotime = time.time()\n",
    "        \n",
    "    papers_l = []\n",
    "    papers_sub = {}\n",
    "    l_list = set(range(999, len(papers)+1000, 1000))\n",
    "    l_list.add(len(papers)-1)\n",
    "    for l, (k, v) in enumerate(papers.items()):\n",
    "        papers_sub[k] = v\n",
    "        if l in l_list:\n",
    "            papers_l.append(papers_sub)\n",
    "            papers_sub = {}\n",
    "    print(len(papers), len(papers_l))\n",
    "    \n",
    "    for j, papers in enumerate(papers_l):\n",
    "        \n",
    "        dic = {}\n",
    "\n",
    "        for k, line in papers.items():\n",
    "\n",
    "            text = line[\"texts\"][\"Ab\"] +\" \"+\" \".join(sum(line[\"Meshhead\"],[])) # 添加mesh词汇\n",
    "            paper = text.lower()\n",
    "            paper = paper.translate(str.maketrans(string.punctuation, \" \"*32,)).replace(\"  \", \" \")\n",
    "            words = set(paper.split(\" \"))\n",
    "\n",
    "            for word in words:\n",
    "        \n",
    "                dic.setdefault(word,[]).extend([k])\n",
    "                \n",
    "        for k, v in dic.items():\n",
    "            invert_dic.setdefault(k,[]).extend(v)\n",
    "        \n",
    "    endtime = time.time()\n",
    "    print(endtime-zerotime)\n",
    "    print(len(invert_dic))\n",
    "    done_l.append(file)\n",
    "    \n",
    "    iter_list = set(range(99, len(files), 100))\n",
    "    iter_list.add(len(files)-1)\n",
    "    if i in iter_list:\n",
    "        \n",
    "        with open(\"words_dic/words_dic_%s.txt\" % len(done_l), 'w') as f:\n",
    "            b = json.dumps(invert_dic)\n",
    "            f.write(b)\n",
    "        with open(\"words_dic/words_dic_done.json\", 'w') as f:\n",
    "            json.dump(done_l, f)\n",
    "        invert_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read: words_dic_800.txt\n",
      "search finished: words_dic_800.txt\n",
      "read: words_dic_100.txt\n",
      "search finished: words_dic_100.txt\n",
      "read: words_dic_900.txt\n",
      "search finished: words_dic_900.txt\n",
      "read: words_dic_200.txt\n",
      "search finished: words_dic_200.txt\n",
      "read: words_dic_1000.txt\n",
      "search finished: words_dic_1000.txt\n",
      "read: words_dic_300.txt\n",
      "search finished: words_dic_300.txt\n",
      "read: words_dic_1100.txt\n",
      "search finished: words_dic_1100.txt\n",
      "read: words_dic_400.txt\n",
      "search finished: words_dic_400.txt\n",
      "read: words_dic_500.txt\n",
      "search finished: words_dic_500.txt\n",
      "read: words_dic_600.txt\n",
      "search finished: words_dic_600.txt\n",
      "read: words_dic_1114.txt\n",
      "search finished: words_dic_1114.txt\n",
      "read: words_dic_700.txt\n",
      "search finished: words_dic_700.txt\n"
     ]
    }
   ],
   "source": [
    "# 检索\n",
    "import json\n",
    "spec_d = {}\n",
    "spec_papers_d = {}\n",
    "for i in [\"Bactology\", \"Mycology\", \"Virology\"]:\n",
    "\n",
    "    with open(\"specs_list/%s.txt\" % (i), \"r\") as f:\n",
    "        specs = [i.replace(\"\\\"\", \"\").replace(\"\\n\", \"\").lower() for i in f.readlines()]\n",
    "    \n",
    "    spec_paper_d = {}\n",
    "    for spec in specs:\n",
    "        spec_paper_d[spec] = []\n",
    "    spec_papers_d[i] = spec_paper_d\n",
    "\n",
    "iter_list = set(range(100, 1114, 100))\n",
    "iter_list.add(1114)\n",
    "for i in iter_list:\n",
    "\n",
    "    with open(\"words_dic/words_dic_%s.txt\" % i, \"r\") as f:\n",
    "        content = f.read()\n",
    "        words_dic = json.loads(content)\n",
    "    print(\"read: words_dic_%s.txt\" % i)\n",
    "    \n",
    "    for k,v in words_dic.items():\n",
    "        words_dic[k] = set(v)\n",
    "        \n",
    "    for spec_type in spec_papers_d:\n",
    "                    \n",
    "        spec_paper_d = spec_papers_d[spec_type]\n",
    "        \n",
    "        for spec in spec_paper_d: \n",
    "\n",
    "            term = spec.lower().split(\" \")\n",
    "            if spec_type in [\"Bactology\", \"Mycology\"]:\n",
    "                if len(term) > 1:\n",
    "                    term_set = words_dic.setdefault(term[0][0], {0}) | words_dic.setdefault(term[0], {0})\n",
    "                else:\n",
    "                    term_set = words_dic.setdefault(term[0], {0})\n",
    "            else:\n",
    "                term_set = words_dic.setdefault(term[0], {0})\n",
    "\n",
    "            for j in range(1,len(term)):\n",
    "                term_set = term_set & words_dic.setdefault(term[j],{0})\n",
    "\n",
    "            spec_paper_d[spec] = spec_paper_d[spec]+ list(term_set)\n",
    "        \n",
    "        spec_papers_d[spec_type] = spec_paper_d\n",
    "\n",
    "    print(\"search finished: words_dic_%s.txt\" % i)\n",
    "\n",
    "    del(words_dic)\n",
    "    with open(\"search_results/search_results.txt\", \"w\") as f:\n",
    "        a = json.dumps(spec_papers_d)\n",
    "        f.write(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611 1602 2994859\n",
      "710 703 579919\n",
      "1270 1094 1842629\n"
     ]
    }
   ],
   "source": [
    "# 统计\n",
    "for i in spec_papers_d:\n",
    "    \n",
    "    spec_count = 0\n",
    "    paper_count = 0\n",
    "    for j in spec_papers_d[i]:\n",
    "        # if len(j.split(\" \"))>1:\n",
    "        paper_count = paper_count + len(spec_papers_d[i][j])\n",
    "        # print(j, len(spec_papers_d[i][j]))\n",
    "        if len(spec_papers_d[i][j]) != 0:\n",
    "            spec_count = spec_count + 1\n",
    "    print(len(spec_papers_d[i]), spec_count,paper_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pubmed_700\n",
      "pubmed_400\n",
      "pubmed_1100\n",
      "pubmed_100\n",
      "pubmed_500\n",
      "pubmed_1000\n",
      "pubmed_900\n",
      "pubmed_600\n",
      "pubmed_800\n",
      "pubmed_1114\n",
      "pubmed_300\n",
      "pubmed_200\n"
     ]
    }
   ],
   "source": [
    "# 索引\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "with open(\"search_results/search_results.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    spec_papers_d = json.loads(content)\n",
    "\n",
    "spec_papers_dic = {}\n",
    "for spec_type in spec_papers_d:\n",
    "    spec_paper_d = {}\n",
    "    for spec in spec_papers_d[spec_type]:\n",
    "        spec_paper_d[spec] = {}\n",
    "    spec_papers_dic[spec_type] = spec_paper_d\n",
    "\n",
    "pubmeds = [i.replace(\".txt\", \"\") for i in os.listdir(\"pubmed_2022_merge\") if \"txt\" in i]\n",
    "\n",
    "for pubmed in pubmeds:\n",
    "    \n",
    "    with open(\"pubmed_2022_merge/%s.txt\" % pubmed, \"r\") as f:\n",
    "        content = f.read()\n",
    "        pubmed_dic = json.loads(content)\n",
    "\n",
    "        print(pubmed)\n",
    "    \n",
    "    for spec_type in spec_papers_d:        \n",
    "        \n",
    "        for spec in spec_papers_d[spec_type]:\n",
    "        \n",
    "            papers_id = set(spec_papers_d[spec_type][spec]) & pubmed_dic.keys()\n",
    "            papers = {}\n",
    "            for paper_id in papers_id:\n",
    "                papers[pubmed_dic[paper_id][\"pmid\"]] = pubmed_dic[paper_id]\n",
    "                \n",
    "            spec_papers_dic[spec_type][spec].update(papers)\n",
    "    \n",
    "    del(pubmed_dic)\n",
    "    \n",
    "    with open(\"search_results/search_results_map.txt\", \"w\") as f:\n",
    "        content = json.dumps(spec_papers_dic)\n",
    "        f.write(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd5a5980a31967086eaababc5338e6a92d5792ba40cc4c910227acd610a4f48f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
